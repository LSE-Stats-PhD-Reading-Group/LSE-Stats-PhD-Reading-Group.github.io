# From Transformer Basics to Alignment: A Unified View of LLM Foundations and Training

This talk presents a unified overview of large language models, spanning foundational concepts, capability formation, and alignment methods. We begin by reviewing the mechanics of autoregressive Transformers, including tokenization, attention, positional representations, and decoding. We next introduce the modern training stack: pre-training, supervised fine-tuning (SFT), RLHF for preference alignment, and RLVR for correctness-driven learning using verifiable reward signals.
